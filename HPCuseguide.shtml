<!--#set var="title" value="HPC Cluster Basic Use Guide"-->
<!--#include virtual="/includes/template-1-openhead.shtml" -->
<!--#include virtual="/includes/template-2-opensidebar.shtml" -->
<!--#include virtual="/includes/template-3-sidebar.shtml" -->
<!--#include virtual="/includes/template-4-mainbody.shtml" -->

<p>
The CHTC has partnered with the UW-Madison <a href="http://aci.wisc.edu/">
Advanced Computing Initiative</a>  
(ACI) in order to provide a dedicated high-performance computing (HPC) cluster meant for large,
 singular computations that use specialized software to achieve internal parallelization of work across 
multiple servers of dozens to hundreds of cores. All other computational work, including that in the form of single or multiple compute jobs that each complete in less than 72 hours on a single node will run more efficiently on our larger high-throughput
 computing (HTC) system (which also includes specialized hardware for extreme memory, GPUs, and other cases), and should not be run on the HPC Cluster.
</br></br>
Before using the campus-shared HPC Cluster or any CHTC computing resource, <b>you will need to 
obtain access</b> by 
filling out the <a href="http://chtc.cs.wisc.edu/form">
Large-Scale Computing Request Form</a> on our website so that our Research Computing Facilitators 
can make sure to match you to the best computing resources (including non-CHTC services).
</br></br>
If you have been using the HPC Cluster already, and believe your research group would benefit from 
purchasing prioritized hardware to add to the HPC Cluster, please see our
<a href="/hpc-buy-in.shtml">
<b>information regarding buy-in options for access to your own priority queue.</b>
</a>
</p>
<p><h1>Contents</h1>
<ol>
  <li><a href="#policy">Cluster Configuration and Policies</a>
    <ul>
    <li><a href="#hardware">Hardware and Partition Configuration</a></li>  
    <li><a href="#head-nodes">Logging In</a></li>
    <li><a href="#files">Data Storage</a></li>
		<li ><a href="#job-scheduling">Partition Configuration and Job Scheduling</a></li>
    </ul></li>
  <li><a href="#use">Basic Use of the Cluster</a>
    <ul>
    <li><a href="#log-in">Log in to the cluster head node</a></li>	
    <li><a href="#software">Software Capabilities</a></li>
    <li><a href="#job-submission">Submitting jobs</a>
		<ul>
			<li><a href="#interactive-job">Requesting an Interactive Job ("int" and "pre" partitions)</a></li>	
		<li><a href="#batch-job">Submitting a Job to the Queue (all partitions)</a></li>
		 </ul></li>
		
		
	
	<li><a href="#queue">Viewing jobs in the queue</a>
	<li><a href="#remove">Removing jobs</a>	
		
	  </ul></li>
  <li><a href="#homedir">Tools for Managing Home Directory Space</a></li>
</ol>



<a name="policy"></a>
<h1>Cluster Configuration and Policies</h1>
<a name="hardware"></a>
<h2>A. Hardware and Partition Configuration</h2>
<p>
The HPC Cluster servers consist of two head nodes and many compute nodes
 ("servers") of memory and multiple CPU cores. There is one queue with access to separate "partitions" 
of hardware, including the largest partitions that are available to
 anyone on campus (for free).</br>
</br>
Our first generation nodes ("univ" partition) each have 16 CPU cores of
 2.2 GHz, and 64 GB of RAM (4 GB per CPU core). Our second generation nodes
 (in the "univ2" partition) each have 20 CPU cores of 2.5 GHz, and 128 GB of
 RAM. All users log in at a head node, and all user files on the shared file
 sytem (Gluster) are accessible on all nodes. Additionally, all nodes are
 tightly networked (56 Gbit/s Infiniband) so they can work together as a
 single "supercomputer", depending on the number of CPUs you specify.
</br></br>
Because the HPC Cluster is specifically intended for singular compute jobs requiring multiple nodes of 
cores (to complete in under a week), users running work in the form of single-node (or smaller) computations will 
be contacted to move such work to our larger and more appropriate HTC System. Otherwise, such numerous smaller jobs impede 
the effectiveness of the HPC Cluster for running larger jobs.
</p>
<a name="head-nodes"></a>
<h2>B. Logging In</h2>
<p>
You may log in to the cluster, submit jobs, and transfer/move data through either head node 
(aci-service-1.chtc.wisc.edu or aci-service-2.chtc.wisc.edu).</p>
<b>DO NOT RUN PROGRAMS ON THE HEAD NODES.</b>  
<p>Simple commands (to compress data, create directories, etc.) that run within a 
few minutes on the head node are okay, but any scripts, software, or other processes 
that perform data manipulations/creation are VERY likely to kill the head nodes, creating 
signficant issues for all active users and CHTC staff.
To ensure proper functioning of the cluster for ALL users, computational work should 
always be run within an interactive session (see below) or 
batch job. <b>CHTC staff reserve the right to kill any long-running or problematic processes 
on the head nodes and/or disable user accounts that violate this policy</b>, 
and users may not be notified of account deactivation. Processes that only occasionally 
perform work (e.g. crontab, etc.) are still a violoation of this policy.</p>

<p>If you are not able to log into the cluster, please contact us at chtc@cs.wisc.edu.</p>

<p>Only ssh connections from an on-campus network are allowed, so
 you will need to first connect to an on-campus server with ssh or Virtual Private Network
 (VPN) before connecting to either HPC head node, when off-campus. (The Division of IT provides the
 <a href="https://it.wisc.edu/services/wiscvpn/">WiscVPN</a>, which will work for users with a UW NetID.)</p>

<a name="files"></a>
<h2>C. Data Storage</h2>

<p>
<b>Data space in the HPC file system is not 
backed-up and should be treated as temporary by users</b>. Only files 
necessary for <i>actively-running</i> jobs should be kept on the file system, 
and files should be removed from the cluster when jobs complete. A copy of any 
essential files should be kept in an alternate, non-CHTC storage location. 
</br></br>
<b>Each user is initially allocated 100 GB of data storage space and a file count quota 
of 50,000 files/directories</b> in their home directory
(/home/<i>username</i>/).  To check how many files and directories you have 
in your home directory (and subdirectories), see the <a href="#homedir">instructions below</a>.  
Increased quotas are available upon email request to
 chtc@cs.wisc.edu.  In your request, please include both size (in GB) and file/directory 
 counts for: 
<ul>
<li>software packages installed in your home directory (if any)</li>
<li>the input and output for a single job</li>
</ul>
If you don't know how many files your installation creates, because it's more than 50,000, 
simply indicate that in your request.
</br></br>
<b>CHTC Staff reserve the right to remove any significant amounts of data
 on the HPC Cluster</b> in our efforts to maintain
 filesystem performance for all users, though we will always first ask users
 to remove excess data and minimize file counts before taking additional action.
</br></br>
<b>Local scratch space</b> of 500 GB is available on each execute node in
 <code>/scratch/local/$USER</code> and is automatically cleaned out upon completion of scheduled
 job sessions (interactive or non-interactive). Local scratch is available on
 the compiling node, aci-service-2, in the same location and should be cleaned
 out by the user upon completion of compiling activities. CHTC staff will otherwise
 clean this location of the oldest files when it reaches 80% capacity.
</p>

<a name="job-scheduling"></a>
<h2>D. Partition Configuration and Job Scheduling</h2>
<p>
The job scheduler on the HPC Cluster is SLURM. You can read more about 
submitting jobs to the queue on 
<a href="https://computing.llnl.gov/linux/slurm/documentation.html">
SLURM's website</a>, but we have provided a simple guide below for getting 
started.
</br></br>
We have provisioned 3 freely-available submission partitions and
 a small set of nodes prioritized for interactive testing. These
 partitions can be thought of as different queues, and are
 selected by the user at the time of job submission.
<h3>Per-user limitations</h3>
<p>
 To promote fairness, <b>there is a 600-core running limit per-user across
 the entire cluster of partitions,</b> with rare exceptions for researchers
 who <i>own</i> more than this number of cores. Additionally, each user
 may only have 10 jobs running at once. Users with many smaller (1-node or
 2-node) jobs will find that they experience better throughput on CHTC's
 high-throughput computing (HTC) system, and can email chtc@cs.wisc.edu to
 get access.
<p>
<table class="gtable">
<tr><th> Partition </th>
    <th> p-name </th>
    <th> # nodes (N)</th>
    <th> t-max </th>
    <th> t-default </th>
    <th> max nodes/job </th>
    <th> cores/node (n)</th>
    <th> RAM/node (GB)</th>
</tr>
<tr>
  <td>University</td>
  <td>univ</td>
  <td>38</td>
  <td>7 days</td>
  <td>1 day</td>
  <td>16</td>
  <td>16</td>
  <td>64</td>
<tr>
  <td>University 2</td>
  <td>univ2</td>
  <td>148</td>
  <td>7 days</td>
  <td>1 day</td>
  <td>16</td>
  <td>20</td>
  <td>128</td>
</tr>
<tr>
  <td>Owners</td>
  <td><i>unique</i></td>
  <td>124</td>
  <td><i>unique</i></td>
  <td><i>unique</i></td>
  <td><i>unique</i></td>
  <td>16 or 20</td>
  <td>64 or 128</td>
</tr>
<tr>
  <td>Interactive</td>
  <td>int</td>
  <td>2</td>
  <td>30 min</td>
  <td>30 min</td>
  <td>1</td>
  <td>16</td>
  <td>64</td>  
</tr>
<tr>
  <td>Pre-emptable (backfill)</td>
  <td>pre</td>
  <td>316</td>
  <td>24 hrs</td>
  <td>4 hrs</td>
  <td>16</td>
  <td>16 or 20</td>
  <td>64 or 128</td>
</tr>
<table>
*note: jobs not requesting a run time will be alotted the default value 
(t-default) for that partition; jobs without a partition indicated will be run 
in the "univ" partition.
</br></br>
The <b>University (univ)</b> partition is available to all UW-Madison 
researchers, and jobs are run without being pre-empted for the 
duration of time requested. This partition is best for running longer (multi-day) 
jobs on any number of CPUs and will always have at least 32 nodes (512 cores), 
but usually much more. 
</br></br>
The <b><i>Owner</i></b> partitions actually consist of multiple group-specific 
partitions for research groups who have paid into the cluster for a set number 
of nodes. Each <i>owner</i> partition will have unique settings, and owned 
nodes are backfilled by jobs from the "pre" queue.
</br></br>
The <b>Interactive (int)</b> partition consists of a few nodes meant for short 
and immediate interactive testing on a single node (up to 16 CPUs, 64 GB RAM). 
There is a specific command to access the "int" partition:</br> 
<code>srun -n16 -N1 -p int --pty bash</code>
</br></br>
The <b>Pre-emptable (pre)</b> partition is under-layed on the entire cluster 
and is meant for more immediate turn-around of shorter and somewhat smaller jobs, 
or for interactive sessions requiring more than the 30-minute limit of the "int" 
partition. 
Pre-emptable jobs will run on any idle nodes (primarily Owner nodes, as the 
University partition is likely to be full), but will be pre-empted by jobs 
of other partitions with priority on those nodes. However, pre-empted jobs will 
be re-queued if originally submitted with an sbatch script (see below).
</br>
<h2>Job Priority Determinations</h2></br>
<b>A. User priority decreases as the user accumulates hours of CPU time</b> over the 
last 21 days, across all queues. This "fair-share" policy means that users who 
have run many/larger jobs in the near-past will have a lower priority, and users 
with little recent activity will see their waiting jobs start sooner. We do NOT 
have a strict "first-in-first-out" queue policy.</br>
<b>B. Job priority increases with job wait time.</b> After the history-based user 
priority calculation in (A), the next most important factor for each job's priority 
is the amount of time that each job has already waited in the queue. For all 
the jobs of a single user, these jobs will most closely follow a 
"first-in-first-out" policy.</br>
<b>C. Job priority increases with job size, in cores.</b> This least important factor 
slightly favors larger jobs, as a means of somewhat countering the inherently 
longer wait time necessary for allocating more cores to a single job.
</p>

<a name="use"></a>
<h1>Basic Use of the Cluster</h1>

<a name="log-in"></a>
<h2>1. Log in to the cluster head node</h2>
<p>
Create an ssh connection to aci-service-1.chtc.wisc.edu using your UW-Madison 
username and associated password.
</p>
<h3>Checking partition availabilty</h3>
<p>
To see partitions that you can submit to, use the following command:
<pre class="term">[alice@service]$ sinfo</pre>
Using the "<code>-a</code>" argument to "<code>sinfo</code>" will show ALL
 partitions.
</p>
<a name="software"></a>
<h2>2. Software Capabilities</h2>
<p>As part of our overall strategy for enabling users through computing, we
 actually encourage users to install and compile their desired software (and
 version), as they wish, within the /home/<i>username</i> location. Compiling, like any
 other computational work is best-performed in an interactive session.  If your compilation 
will take more than the 30 minutes allowed on our interactive ("int") partition, alter the 
interactive job command below to submit to "univ" or "univ2" to have more time.  Please email 
chtc@cs.wisc.edu if you can't find the compiler you need or have other issues.
</br></br>
<b>For more specific details on compiling and running MPI code</b></br>
Please see our <a href=MPIuseguide>HPC Cluster MPI Use Guide</a> for information about the availability
of specific libraries and how to load modules for them. 
</br>
</p>
<a name="job-submission"></a>
<h2>3. Submitting jobs</h2>
<a name="interactive-job"></a>
<p>
<b>A. Requesting an Interactive Job ("int" and "pre" partitions)</b></br>
You may request up to a full node (16 CPUs, 64 GB RAM) when requesting an 
interactive session in the "int" partition. Interactive sessions on the "int" 
partition are allowed for 30 minutes, but you may request less time 
(see the below example). Sessions in the "pre" partition are limited according to 
the "Partition" table above, but are potentially subject to interruption.
<pre class="term">[alice@service]$ srun -n16 -N1 -p int --pty bash</pre>
The above example indicates a request for 16 CPUs (<code>-n16</code>) on a 
single node (<code>-N1</code>) in the "int" partition (<code>-p int</code>), and 
"<code>-t 15</code>" would indicate a request for 15 minutes, if desired rather than the 
30-minute default. After the interactive shell is created to a compute node with 
the above command, you'll have access to files on the shared file system 
and be able to execute code interactively as if you had 
directly logged in to that node. <b>It is important to exit the interactive shell 
when you're done working by typing <code>exit</code></b>.
</br></br>
<a name="batch-job"></a>
<b>B. Submitting a Job to the Queue (all partitions)</b></br>
To submit jobs to the queue for a given partition such that a connection 
to the jobs is not maintained, you should use <code>sbatch</code> submission. 
You will first want to create an <code>sbatch</code> script, which is
 is essentially just a shell script (sh, bash, etc.) with <code>#SBATCH</code>
 descriptor lines.</br>
The following example requests a job slot with 16 CPU cores
 on each of 2 nodes (32 cores total) for 4 hours and 30 minutes:
</br>
<pre class="sub">
#!/bin/sh
#This file is called submit-script.sh
#SBATCH --partition=univ		# default "univ", if not specified
#SBATCH --time=0-04:30:00		# run time in days-hh:mm:ss
#SBATCH --nodes=2			# require 2 nodes
#SBATCH --ntasks-per-node=16            # (by default, "ntasks"="cpus")
#SBATCH --mem-per-cpu=4000		# RAM per CPU core, in MB (default 4 GB/core)
#SBATCH --error=job.%J.err
#SBATCH --output=job.%J.out
#Make sure to change the above two lines to reflect your appropriate
# file locations for standard error and output

#Now list your executable command (or a string of them).
# Example for non-SLURM-compiled code:
module load mpi/gcc/openmpi-1.6.4
mpirun -n 32 /home/<i>username</i>/mpiprogram
</pre>
You can then submit the script with the following command:
<pre class="term">
[alice@service]$ sbatch <i>submit-script.sh</i>
</pre>
Other lines that you may wish to add to your script for specifying a number of total
 tasks (equivalent to "cores" by default),
 desired CPU cores per task (for multiple CPU cores per MPI task),
 or total RAM per node are:
<pre class="sub">
#SBATCH --mem=4000         # RAM per node, in MB (default 64000/node, max values in partition table)
#SBATCH --ntasks=32        # total number of "tasks" (cores) requested
#SBATCH --cpus-per-task=1  # default "1" if not specified
</pre>
In any case, it is important to make sure that your request fits within the hardware
 configuration of your chosen partition.
</br></br>
<b>C. Using srun and salloc</b></br>
In early tests with the cluster, we encouraged running non-interactive jobs
 with SLURM's <code>srun</code> and <code>salloc</code> commands
 <i>without</i> an sbatch script; however, doing so 
creates and requires a persistent connection to your job as it runs, and 
interrupted jobs are not re-queued if submitted this way (even when using
 the "pre" partition). You are welcome to submit jobs in 
these modes according to 
<a href="https://computing.llnl.gov/linux/slurm/quickstart.html">SLURM's user 
guide</a>, which has some awesome advanced features for complex MPI configurations.</br>
<b>Please remember to indicate partition and run time with the -p and 
-t flags, respectively</b> (see the interactive job command, above, for an 
example using these flags).
</p>
<a name="queue"></a>
<h2>4. Viewing jobs in the queue</h2>
<p>
To view <i>your</i> jobs in the SLURM queue, enter the following:
<pre class="term">[alice@service]$ squeue -u <i>username</i></pre>
Issuing <code>squeue</code> alone 
will show all user jobs in the queue. You can view all jobs for a particular 
partition with "<code>squeue -p univ</code>"
</p>

<a name="remove"></a>
<h2>5. Removing jobs</h2>
<p>
After running <code>squeue</code>, you can kill and/or remove your job from
 the queue with the following:
<pre class="term">[alice@service]$ scancel <i>job#</i></pre>
where <code><i>job#</i></code> is the number shown for your job in the <code>
squeue</code> output.
</p>
<a name="homedir"></a>
<h1>Tools for Managing Home Directory Space</h1>

<h2>1. Checking Home Directory Usage</h2>

<p>In order to check how many files and directories are contained in your 
home directory, as well as the total amount of space used, we recommend using 
the Linux tool <code>ncdu</code>. 

<p>To check data usage and file counts, run <code>ncdu</code> from within
the directory you'd like to query. Example:
<pre class="term">
[alice@service]$ cd /home/alice
[alice@service]$ ncdu
</pre>
When <code>ncdu</code> has finished running,
the output will give you a total file count and allow you to navigate between
subdirectories for even more details. Type <code>q</code> when you're ready to
exit the output viewer. More info here:
<a href="https://lintut.com/ncdu-check-disk-usage/">https://lintut.com/ncdu-check-disk-usage/</a>

<p>The command <code>du</code> can also be used to examine the size of 
directories: 
<pre class="term">
[alice@service]$ df -h /home/alice
</pre>
</p>

<h2>2. "Disappearing" Files</h2>

<p>Due to a bug in the file system program used on the cluster, certain 
files can sometimes "disappear".  The file is still there, but you can't see it when 
you look for files using the <code>ls</code> command.  People frequently notice this 
error when: 
<ul>
<li>Trying to delete a folder and getting an error message like: 
  <pre class="term">rm: cannot remove 'folder/': Directory not empty </pre>
</li>
<li>A job looks like it completed successfully, but you can't see all the files that should be there.</li>
</ul>
</p>

<p>If you know the name of the file that should be there, you can try to read it 
by providing the full file name to a file reader like <code>less</code> or <code>cat</code>.  You 
can also remove the file directly if you know its name.  If you'd like to make the file 
visible again, you can attempt to do so by going through the following sequence of 
commands: 
<pre class="term">
[alice@service]$ cp <i>filename</i> /home/<i>username</i>
[alice@service]$ rm <i>filename</i>
[alice@service]$ mv /home/<i>username</i>/<i>filename</i> .
</pre>
</p>

<p>If you are trying to remove files and <b>can't remember the file's name</b>, create a 
directory called "to_delete" in your home directory and move the problematic directories 
there.  Then send an email to chtc@cs.wisc.edu and we can remove the contents of 
that folder. 
</p>

<!--#include virtual="/includes/template-5-finish.shtml" -->
