---
layout: default
title: Running MPI Jobs in CHTC
---

This guide describes when and how to run multi-core jobs, programmed 
with MPI, in CHTC's high throughput compute (HTC) system.  

<p><b>To best understand the below information, users should already have 
an understanding of:</b>
</p>

<ul>
	<li>Using the command line to: navigate within directories, 
	create/copy/move/delete files and directories, and run their 
	intended programs (aka "executables").</li>
	<li><a href="http://chtc.cs.wisc.edu/helloworld.shtml">The CHTC's 
	Intro to Running HTCondor Jobs</a></li>
</ul>

<h1>Overview</h1>

<p>Before you begin, review our below discussion of <a href="#require">MPI requirements 
and use 
cases</a>, to make sure that our multi-core MPI capabilities are the right 
solution for your computing problem. If you have any questions, 
contact a CHTC facilitator at 
<a href="chtc@cs.wisc.edu">chtc@cs.wisc.edu</a>.  </p>

<p>Once you know that you need to run multi-core jobs that use  
MPI on our HTC system, you will 
need to do the following: 
<ol>
	<li> <a href="#compile">Compile your code using our MPI module system</a></li>
	<li> <a href="#script">Create a script to that 
	loads the MPI module you used for compiling, and
	then runs your code</a></li>
	<li> <a href="#submit">Make sure your submit file has 
	certain key requirements</a></li>
</ol>
</p>

<blockquote><b>Software Transition</b> <br>

<p>We are transitioning to a new set of MPI modules during March 2020. <b>Jobs should start 
transitioning to using the new modules NOW</b>; the old MPI modules will be retired by 
April 20, 2020. See below for more details.</p></blockquote>


<a name="require"></a>
<h1>A. Requirements and Use Cases</h1>

<p>Most jobs on CHTC's HTC system are 
run on one CPU (sometimes called a "processor", or "core") 
and can be executed without any special system libraries.  
However, in some cases, it may be advantageous to run a single program on 
multiple CPUs (also called multi-core), in order to speed up single computations 
that cannot be broken up as independent jobs.  
If you have questions about the advantages and disadvantages of running 
multi-core jobs versus single-core jobs, contact one of CHTC's facilitators at 
<a href="chtc@cs.wisc.edu">chtc@cs.wisc.edu</a>.</p>

<p>Running on multiple CPUs can be enabled by the parallel programming standard 
MPI. For MPI jobs to
compile and run, CHTC has a certain set of MPI tools installed 
to a shared location, that can be accessed from within jobs. </p>

<a name="view"></a>
<h1>B. View MPI Modules on the HTC System</h1>

<p>MPI tools are accessible on the HTC system through software "modules", which 
are tools to access and activate a software installation. To 
see which MPI packages are supported, you can type the following command
on a an HTC submit server:</p>

<pre class="term">[alice@submit]$ module avail</pre>

<p><b>IMPORTANT NOTICE</b>: Note that the <code>module avail</code> 
command currently shows two sets of MPI (and other software) modules. The first set, listed under the header
<code>/software/chtc/modules</code> are new and should be used going forward. The 
old MPI modules are listed under <code>/etc/modulefiles</code> and will 
be retired on <b>April 20</b>; jobs that use these modules should be transitioned 
to new modules by then. <b>If you need a module that hasn't yet been added to the new 
group, please email us at chtc@cs.wisc.edu.</b> </p>

<p>Your software may require newer versions of MPI libraries than those 
available via our modules.  If this is the case, send an email to <a href="chtc@cs.wisc.edu">chtc@cs.wisc.edu</a>, 
to find out if we can install that library into the module system.</p>

<h1>C. Submitting MPI jobs</h1>

<a name="compile"></a>
<h2>1. Compile MPI Code</h2>

<p>You can compile your program
yourself within an interactive job on one of our compiling servers. Do not 
compile code on the submit server, as doing so may cause performance issues. 
The interactive job is essentially a regular HTCondor job, but <i>without</i> 
an executable; <b>you</b> are the one running the commands instead (in this case, 
to compile the program). 
</p>

<p><b>Instructions for submitting an interactive build/compile job</b> are here: 
<a href="http://chtc.cs.wisc.edu/inter-submit.shtml">http://chtc.cs.wisc.edu/inter-submit.shtml</a> <br>
The only line in the submit file that you need to change is <code>transfer_input_files</code> 
to reflect all the source files on which your program depends. Otherwise, go through 
the steps described in that guide until immediately after running <code>condor_submit -i</code>.
</p>

<p>Once your interactive job begins on one of our compiling servers, you 
can confirm which MPI modules are available to you by typing:</p>

<pre class="term">[alice@build]$ module avail</pre>

<p>Choose the module you want to use and load 
it with the following command: </p>
<pre class="term">[alice@build]$ module load <i>mpi_module</i></pre>
<p>where <code>mpi_module</code> is replaced with the name of the
MPI module you'd like to use.  </p>

<blockquote><b>IMPORTANT NOTICE</b>: Make sure you are using a current MPI module; shown in 
the first section of modules under the <code>/software/chtc/modules</code> heading.</blockquote>
 
<p>After loading the module, compile your program. If your program is organized 
in directories, <b>make sure to create a tar.gz file of anything you want copied 
back to the submit server</b>. Once typing <code>exit</code> the interactive job 
will end, and any *files* created during the interactive job will be copied back 
to the submit location for you. 

<p>If your MPI program is especially large (more than 100 MB, compiled), 
or if it can <i>only</i> run from the exact location to which it was installed, 
you may also need to take advantage of CHTC's shared software location or our public 
web proxy. Email 
CHTC's Research Computing Facilitators at chtc@cs.wisc.edu if this is the case.</p>

<a name="script"></a>
<h2>2. Script For Running MPI Jobs</h2>

<p>To run your newly compiled program within a job, you need to write a 
script that loads an MPI module and then runs the program, like so: </p>

<pre class="file">
#!/bin/bash

# Command to enable modules, and then load an appropriate MP/MPI module
. /etc/profile.d/modules.sh
module load <i>mpi_module</i>

# Untar your program installation, if necessary
tar -xzf <i>my_install.tar.gz</i>

# Command to run your OpenMP/MPI program
# (This example uses mpirun, other programs
# may use mpiexec, or other commands)
mpirun -np 8 <i>./path/to/myprogram</i>
</pre>

<p>Replace <code>mpi_module</code> with the name of the module you used 
to compile your code, <code>myprogram</code> with the name of your program, and 
<code>X</code> with the number of CPUs you want the program to use.  There may 
be additional options or flags necessary to run your particular program; make sure 
to check the program's documentation about running multi-core processes.
</p>

<a name="submit"></a>
<h2>3. Submit File Requirements</h2>

<p>There are several important requirements to consider when writing 
a submit file for multicore jobs.  They are shown in the sample submit 
file below and include: </p>

<ul>
	<li><b>Require access to MPI modules.</b> There is a special requirements statement
	that ensures a computer has access to the shared software location maintained by 
	CHTC. 
	<pre class="sub">
requirements = (HasCHTCSoftware == true)
</pre></li>
	<li>Use the <b>getenv = true</b> statement to set up the job's 
	running environment.
	<pre class="sub">
getenv = true
</pre></li>
	<!--<li><b>Require Scientific Linux 6.</b> At the moment, our MPI modules are 
	only fully functional on our older operating system, Scientific Linux 6.</li>-->
	<li><b>Request *accurate* CPUs and memory</b> Run at least one test job 
	and look at the log file produced by HTCondor to determine how much memory 
	and disk space your multi-core jobs actually use. Requesting too much memory 
	will cause two issues: your jobs will match more slowly and they will be wasting 
	resources that could be used by others. Also, the fewer CPUs your jobs require,
 	the sooner you'll have more jobs running. Jobs requesting 16 CPUs or less 
	will do best, as nearly all of CHTC's servers have at least that many, but 
	you can request and use up to 36 CPUs per job.</li>
	<li><b>The script you wrote above (shown as <code>run_mpi.sh</code> below)
	should be your submit file "executable"</b>, and your compiled program and any 
	 files should be listed in <b>transfer_input_files</b>.</li>
</ul>

<p>A sample submit file for multi-core jobs is given below: </p>

<pre class="sub">
# multicore.sub
# A sample submit file for running a single multicore (8 cores) job

## General submit file options
universe = vanilla
log = <i>mc_$(Cluster)</i>.log
output = <i>mc_$(Cluster)</i>.out
error = <i>mc_$(Cluster)</i>.err

## Submit file options for running your program
executable = <i>run_mpi</i>.sh
# arguments = (if you want to pass any to the shell script)
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = <i>input_files</i>, <i>myprogram</i>

getenv = true
# Requirement for accessing new set of modules
requirements = ( Target.HasChtcSoftware == true ) 
# Old requirement for previous set of MPI modules
# These modules will not be available after April 20
# requirements = ( Target.HasModules == true )

## Request resources needed by your job
request_cpus = <i>8</i>
request_memory = <i>8GB</i>
request_disk = <i>2GB</i>

queue
</pre>

<p>After the submit file is complete, you can 
submit your jobs using <code>condor_submit</code>.  </p>


